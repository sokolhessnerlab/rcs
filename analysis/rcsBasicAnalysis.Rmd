---
title: "RCS Basic Analysis"
author: "Hayley Brooks"
date: "2022-10-12"
output: html_document
---

```{r setup, include=FALSE}
rm(list=ls())

library('config')
config = config::get()

library('lmerTest')

setup_source = file.path(config$path$code_files$dataSetUp) # run our set up script (which loads all the data)
source(setup_source) #, local = knitr::knit_global())
```

#### How many participants are in each condition/order?
```{r conditions, echo=FALSE}

cat("participants in natural-natural condition:",sum(rcsSubLevelWide_clean$condCode==1), "\nparticipants in natural-strategy condition:",sum(rcsSubLevelWide_clean$condCode==2), "\nparticipants in strategy-natural condition:",sum(rcsSubLevelWide_clean$condCode==3),"\nparticipants in strategy-strategy condition:", sum(rcsSubLevelWide_clean$condCode==4))


condcode1 = rcsSubLevelWide_clean$subID[rcsSubLevelWide_clean$condCode==1]
condcode2 = rcsSubLevelWide_clean$subID[rcsSubLevelWide_clean$condCode==2]
condcode3 = rcsSubLevelWide_clean$subID[rcsSubLevelWide_clean$condCode==3]
condcode4 = rcsSubLevelWide_clean$subID[rcsSubLevelWide_clean$condCode==4]
```


#### Average change in risk-taking across conditions
```{r, risk-taking-across-rounds, echo=FALSE}

# differences in pgam from round 1 to round 2:
pgamDiff = rcsSubLevelWide_clean$round1_pgamble - rcsSubLevelWide_clean$round2_pgamble

cat("\nChange in risk-taking from round1 to round 2 \n(round 1-round2)\n",mean(pgamDiff))

# pgamble diff between round 1 and 2 across conditions
# control, control

cat("\n\nDifference in risk-taking across rounds \nnatural(round1) - natural (round2)\ndiff p(gamble)",mean(rcsSubLevelWide_clean$round1_pgamble[rcsSubLevelWide_clean$subID %in% as.numeric(condcode1)] - rcsSubLevelWide_clean$round2_pgamble[rcsSubLevelWide_clean$subID %in% as.numeric(condcode1)]))


# control, strategy
cat("\n\nDifference in risk-taking across rounds \nnatural(round1) - strategy (round2)\ndiff p(gamble)",mean(rcsSubLevelWide_clean$round1_pgamble[rcsSubLevelWide_clean$subID %in% as.numeric(condcode2)] - rcsSubLevelWide_clean$round2_pgamble[rcsSubLevelWide_clean$subID %in% as.numeric(condcode2)]))

# strategy, control
cat("\n\nDifference in risk-taking across rounds \nstrategy(round1) - natural (round2)\ndiff p(gamble)",mean(rcsSubLevelWide_clean$round1_pgamble[rcsSubLevelWide_clean$subID %in% as.numeric(condcode3)] - rcsSubLevelWide_clean$round2_pgamble[rcsSubLevelWide_clean$subID %in% as.numeric(condcode3)]))

# strategy, strategy
cat("\n\nDifference in risk-taking across rounds \nstrategy(round1) - strategy (round2)\ndiff p(gamble)",mean(rcsSubLevelWide_clean$round1_pgamble[rcsSubLevelWide_clean$subID %in% as.numeric(condcode4)] - rcsSubLevelWide_clean$round2_pgamble[rcsSubLevelWide_clean$subID %in% as.numeric(condcode4)]))

```

#### Plot change in risk-taking across rounds and conditions
##### More risk taking in strategy condition (regardless of order)
```{r plot-basics, echo=FALSE}

#pdf("/Users/hayley/Desktop/shenhavLabTalk/risk-taking_rounds.pdf")
# plot(RDMqualityCheck$pgambleRound1[-c(12,15)]-RDMqualityCheck$pgambleRound2[-c(12,15)], ylab ="pGamble(round 1 - round 2)", xlab="participant", main="change in risk-taking across rounds", axes=F, pch=16, col="maroon", cex=1.5, ylim=c(-.4,.4), xlim=c(1,24))
# abline(h=0, col="grey", lty="dashed")
# axis(1, at = 1:nSub, labels = 1:nSub, lwd=3)
# axis(2, at = seq(-.4, .4, .1), lwd=3)
#dev.off()


# create a variable for condition code
# 1 = control, control
# 2 = control, strategy
# 3 = strategy, control
# 4 = strategy, strategy


par(pty="s")
plot(rcsSubLevelWide_clean$round1_pgamble[rcsSubLevelWide_clean$condCode==1], rcsSubLevelWide_clean$round2_pgamble[rcsSubLevelWide_clean$condCode==1], asp=1,col = rgb(red=0, green=0, blue=1, alpha = 0.4), pch=16, ylim=c(0,1), xlim=c(0,1), ylab="p(gamble round 1)", xlab="p(gamble round 2)", axes=F, cex=2)

abline(a = 0, b=1, lty="dashed", col="darkgrey",lwd=5)

points(rcsSubLevelWide_clean$round1_pgamble[rcsSubLevelWide_clean$condCode==2], rcsSubLevelWide_clean$round2_pgamble[rcsSubLevelWide_clean$condCode==2], col = rgb(red=0, green=1, blue=0, alpha = 0.4),asp=1, pch=16,cex=2)

points(rcsSubLevelWide_clean$round1_pgamble[rcsSubLevelWide_clean$condCode==3], rcsSubLevelWide_clean$round2_pgamble[rcsSubLevelWide_clean$condCode==3], asp=1, col = rgb(red=1, green=0, blue=0, alpha = 0.4), pch=16, cex=2)


points(rcsSubLevelWide_clean$round1_pgamble[rcsSubLevelWide_clean$condCode==4], rcsSubLevelWide_clean$round2_pgamble[rcsSubLevelWide_clean$condCode==4], asp= 1,col = rgb(red=.5, green=0, blue=.5, alpha = 0.4), pch=16, cex=2)
axis(1, lwd=6)
axis(2, lwd=6)
legend("bottomright", legend=c("nat-nat","nat-strat", "strat-nat" ,"strat-strat"), pch=16, bty="n", col=c("blue", "green", "red", "purple"), cex=1)





plot(c(rcsSubLevelWide_clean$round1_pgamble[rcsSubLevelWide_clean$condCode==2]- rcsSubLevelWide_clean$round2_pgamble[rcsSubLevelWide_clean$condCode==2], rcsSubLevelWide_clean$round1_pgamble[rcsSubLevelWide_clean$condCode==3]- rcsSubLevelWide_clean$round2_pgamble[rcsSubLevelWide_clean$condCode==3]), pch=16, ylab="Risk-taking\n (round 1 - round 2)", xlab="participant", ylim=c(-.5,.5),cex= 2,col=c(rep("green",length(condcode2)), rep("red", length(condcode3))), axes=F)
axis(1, at =1:sum(rcsSubLevelWide_clean$condCode %in% c(2,3)), labels = as.numeric(c(rcsSubLevelWide_clean$subID[rcsSubLevelWide_clean$condCode==2], rcsSubLevelWide_clean$subID[rcsSubLevelWide_clean$condCode==3])), lwd=6, cex.axis=1)
axis(2, lwd=6, cex=1.5)
abline(h=0, col="darkgrey", lty="dashed", lwd=4)

legend("bottomright", legend=c("nat-strat", "strat-nat"), pch=16, bty="n", col=c("green","red"), cex=1)




```


#### Post round difficulty ratings
```{r instDifficulty-ratings}
# summary stats for diffciculty ratings in each condition and order
# participant generally felt that strategy was more difficult, on average.
summary(rcsSubLevelLong_clean$instDifficult[rcsSubLevelLong_clean$strategy==0])
summary(rcsSubLevelLong_clean$instDifficult[rcsSubLevelLong_clean$strategy==1])

# for participants who did natural condition both rounds, difficulty ratings is on average, higher for second round
natNatRound1difficulty = rcsSubLevelWide_clean$round1_instDifficult[rcsSubLevelWide_clean$condCode==1]
natNatRound2difficulty = rcsSubLevelWide_clean$round2_instDifficult[rcsSubLevelWide_clean$condCode==1]

summary(natNatRound1difficulty)
summary(natNatRound2difficulty)


# for participants who did natural then strategy, difficulty rating is on average, higher for second round (strategy harder)
natStratRound1difficulty = rcsSubLevelWide_clean$round1_instDifficult[rcsSubLevelWide_clean$condCode==2]
natStratRound2difficulty = rcsSubLevelWide_clean$round2_instDifficult[rcsSubLevelWide_clean$condCode==2]

summary(natStratRound1difficulty)
summary(natStratRound2difficulty)

# for participants who did strategy then natural, difficulty rating is on average, higher for first round (strategy harder)
stratNatRound1difficulty = rcsSubLevelWide_clean$round1_instDifficult[rcsSubLevelWide_clean$condCode==3]
stratNatRound2difficulty = rcsSubLevelWide_clean$round2_instDifficult[rcsSubLevelWide_clean$condCode==3]

summary(stratNatRound1difficulty)
summary(stratNatRound2difficulty)

# for participants who did strategy then strategy, difficulty rating is on average, higher for first round 
stratStratRound1difficulty = rcsSubLevelWide_clean$round1_instDifficult[rcsSubLevelWide_clean$condCode==4]
stratStratRound2difficulty = rcsSubLevelWide_clean$round2_instDifficult[rcsSubLevelWide_clean$condCode==4]

summary(stratStratRound1difficulty)
summary(stratStratRound2difficulty)

# these results suggest that we could use the round 1 ratings for natural vs strategy to compare 

pdf(file.path(config$path$directory, config$path$shlab_figures,'InstDifficultyRatingPlots.pdf'))
par(pty="s", mfrow= c(2,2))
plot(natNatRound1difficulty, natNatRound2difficulty, ylim=c(0,100), xlim=c(0,100), ylab="Act natural Round 2", xlab="Act natural Round 1", pch=16, cex = 1.5, col="blue")
abline(a=0, b=1, col="grey")

plot(natStratRound1difficulty, natStratRound2difficulty, ylim=c(0,100), xlim=c(0,100), ylab="Strategy Round 2", xlab="Act natural Round 1", pch=16, cex = 1.5, col="green")
abline(a=0, b=1, col="grey")

plot(stratNatRound1difficulty, stratNatRound2difficulty, ylim=c(0,100), xlim=c(0,100), ylab="Act Natural Round 2", xlab="Strategy Round 1", pch=16, cex = 1.5, col="red")
abline(a=0, b=1, col="grey")

plot(stratStratRound1difficulty, stratStratRound2difficulty, ylim=c(0,100), xlim=c(0,100), ylab="Strategy Round 2", xlab="Strategy Round 1", pch=16, cex = 1.5, col="purple")
abline(a=0, b=1, col="grey")

mtext("Instruction Difficulty Ratings", side = 3, line = - 2, outer = TRUE)
dev.off()

```


#### Post round how often ratings
```{r instFrequency-ratings}
# summary stats for how often ratings in each condition and order

# participants rated, on average, that they were able to follow instructions about 75% of the time with a slightly higher rating reported for the control condition
summary(rcsSubLevelLong_clean$instHowOften[rcsSubLevelLong_clean$strategy==0])
summary(rcsSubLevelLong_clean$instHowOften[rcsSubLevelLong_clean$strategy==1])


# for participants who did natural condition both rounds, participants felt they followed task instructions roughly the same amount of time across rounds
natNatRound1howOften = rcsSubLevelWide_clean$round1_instHowOften[rcsSubLevelWide_clean$condCode==1]
natNatRound2howOften = rcsSubLevelWide_clean$round2_instHowOften[rcsSubLevelWide_clean$condCode==1]

summary(natNatRound1howOften)
summary(natNatRound2howOften)


# for participants who did natural then strategy, participants report implementing strategy condition less often than acting natural (rather large difference)
natStratRound1howOften = rcsSubLevelWide_clean$round1_instHowOften[rcsSubLevelWide_clean$condCode==2]
natStratRound2howOften = rcsSubLevelWide_clean$round2_instHowOften[rcsSubLevelWide_clean$condCode==2]

summary(natStratRound1howOften)
summary(natStratRound2howOften)

# for participants who did strategy then natural, participants report implementing act natural instructions slightly more often than strategy (smaller difference than above)
stratNatRound1howOften = rcsSubLevelWide_clean$round1_instHowOften[rcsSubLevelWide_clean$condCode==3]
stratNatRound2howOften = rcsSubLevelWide_clean$round2_instHowOften[rcsSubLevelWide_clean$condCode==3]

summary(stratNatRound1howOften)
summary(stratNatRound2howOften)

# for participants who did strategy then strategy, participants rated following instructions more, on average, in the first round, relative to second round.
stratStratRound1howOften = rcsSubLevelWide_clean$round1_instHowOften[rcsSubLevelWide_clean$condCode==4]
stratStratRound2howOften = rcsSubLevelWide_clean$round2_instHowOften[rcsSubLevelWide_clean$condCode==4]

summary(stratStratRound1howOften)
summary(stratStratRound2howOften)

# although people feel like natural and strategy are different in difficulty, it looks like they are able to do implement them roughly the same amount of time (here we are seeing averages for all 70-80%)

pdf(file.path(config$path$directory, config$path$shlab_figures,'InstFrequencyRatingPlots.pdf'))
par(pty="s", mfrow= c(2,2))
plot(natNatRound1howOften, natNatRound2howOften, ylim=c(0,100), xlim=c(0,100), ylab="Act natural Round 2", xlab="Act natural Round 1", pch=16, cex = 1.5, col="blue")
abline(a=0, b=1, col="grey")

plot(natStratRound1howOften, natStratRound2howOften, ylim=c(0,100), xlim=c(0,100), ylab="Strategy Round 2", xlab="Act natural Round 1", pch=16, cex = 1.5, col="green")
abline(a=0, b=1, col="grey")

plot(stratNatRound1howOften, stratNatRound2howOften, ylim=c(0,100), xlim=c(0,100), ylab="Act Natural Round 2", xlab="Strategy Round 1", pch=16, cex = 1.5, col="red")
abline(a=0, b=1, col="grey")

plot(stratStratRound1howOften, stratStratRound2howOften, ylim=c(0,100), xlim=c(0,100), ylab="Strategy Round 2", xlab="Strategy Round 1", pch=16, cex = 1.5, col="purple")
abline(a=0, b=1, col="grey")

mtext("Instruction Frequency Ratings", side = 3, line = - 2, outer = TRUE)
dev.off()

```

#### average RT across conditions

```{r RT-conditions}

# On average, participants are slower to respond in the strategy condition (difference is very small)
summary(rcsSubLevelLong_clean$RT[rcsSubLevelLong_clean$strategy==0])
summary(rcsSubLevelLong_clean$RT[rcsSubLevelLong_clean$strategy==1])


# for participants who did natural then natural, they are, on average, faster in the second round
natNatRound1rt = rcsSubLevelWide_clean$round1_RT[rcsSubLevelWide_clean$condCode==1]
natNatRound2rt = rcsSubLevelWide_clean$round2_RT[rcsSubLevelWide_clean$condCode==1]

summary(natNatRound1rt)
summary(natNatRound2rt)

# for participants who did natural then strategy, they are slightly faster in the second round (strategy)
natStratRound1rt = rcsSubLevelWide_clean$round1_RT[rcsSubLevelWide_clean$condCode==2]
natStratRound2rt = rcsSubLevelWide_clean$round2_RT[rcsSubLevelWide_clean$condCode==2]

summary(natStratRound1rt)
summary(natStratRound2rt)


# for participants who did strategy then natural, they are slightly slower in the first round (strategy) relative to second (natural)
stratNatRound1rt = rcsSubLevelWide_clean$round1_RT[rcsSubLevelWide_clean$condCode==3]
stratNatRound2rt = rcsSubLevelWide_clean$round2_RT[rcsSubLevelWide_clean$condCode==3]

summary(stratNatRound1rt)
summary(stratNatRound2rt)


# for participants who did strategy then strategy, they are faster in the second round
stratStratRound1rt = rcsSubLevelWide_clean$round1_RT[rcsSubLevelWide_clean$condCode==4]
stratStratRound2rt = rcsSubLevelWide_clean$round2_RT[rcsSubLevelWide_clean$condCode==4]

summary(stratStratRound1rt)
summary(stratStratRound2rt)

# when repeating instructions, participants are faster in second round. When switching, the difference is very small.


pdf(file.path(config$path$directory, config$path$shlab_figures,'rtAcrossRndsStratPlots.pdf'))
par(pty="s", mfrow= c(2,2))
plot(natNatRound1rt, natNatRound2rt, ylim=c(0,3), xlim=c(0,3), ylab="Act natural Round 2", xlab="Act natural Round 1", pch=16, cex = 1.5, col="blue")
abline(a=0, b=1, col="grey")

plot(natStratRound1rt, natStratRound2rt, ylim=c(0,3), xlim=c(0,3), ylab="Strategy Round 2", xlab="Act natural Round 1", pch=16, cex = 1.5, col="green")
abline(a=0, b=1, col="grey")

plot(stratStratRound1rt, stratStratRound2rt, ylim=c(0,3), xlim=c(0,3), ylab="Act Natural Round 2", xlab="Strategy Round 1", pch=16, cex = 1.5, col="red")
abline(a=0, b=1, col="grey")

plot(stratStratRound1rt, stratStratRound2rt, ylim=c(0,3), xlim=c(0,3), ylab="Strategy Round 2", xlab="Strategy Round 1", pch=16, cex = 1.5, col="purple")
abline(a=0, b=1, col="grey")

mtext("Average Reaction Time", side = 3, line = - 2, outer = TRUE)
dev.off()


```

Results above establish some similarities across that conditions: people take similar lengths of time and how often people are able to follow instructions is different (but similar ranges of values) even though people do think the strategy is more difficult. 


### Generalized linear mixed effects models
#### Trial-level
##### There are a couple of possibilities here. Account for round and strategy in trial-level models or leave out to help detect interactions in subsequent (context models)
```{r trial-level-glmer}

# model with gain, safe and EV level (magnitude)
model1_trialLev =glmer(choice ~ 1 +  gainScaled + safeScaled + evLevScaled + (1|subID), data=rdmDFclean , family = "binomial")
summary(model1_trialLev)
# risk-taking increases as gain amounts increases and decreases as safe values increase and magnitude increases
# AIC = 25023.0

# model with just round
model2_trialLevRound= glmer(choice ~ 1 + gainScaled + safeScaled + evLevScaled + roundRecode + (1|subID), data=rdmDFclean , family = "binomial")
summary(model2_trialLevRound)
# AIC = 25024.6
# gain, safe, EV as above
# no effect of round 

# model with round and strategy (and interaction)
model3_trialLevRoundStrat= glmer(choice ~ 1 + gainScaled + safeScaled + evLevScaled + roundRecode*strategyRecode + (1|subID), data=rdmDFclean , family = "binomial")
summary(model3_trialLevRoundStrat)
# AIC = 25006.5
# gain, safe, EV as above
# no main effect of round 
# main effect of strategy (more risk-taking in strategy condition)
# positive interaction between round and strategy 
  # this interaction means that the effect of each condition is stronger in round 2 (i.e. more risk-taking for strategy, esp in round 2 and less risk-taking in natural, esp round 2)


# Including round but not strategy in trial-model makes the subsequent analyses weird because there is a (potential) interaction between round and strategy. We either want to include both or none. Below, we will two analyses using predicted values (i.e. residuals) from model with just gain, safe, and EV and model with both round and strategy
```

```{r plot-strategy-round-effect}

# using estimates from model3_trialLevRoundStrat:
model3summary = summary(model3_trialLevRoundStrat)

conditionBeta = model3summary$coefficients[6]
roundBeta = model3summary$coefficients[5]
condRoundIntxnBeta =model3summary$coefficients[7]

rounds = c(-1,-1,1,1); # round 1, round 1, round 2, round 2
conditions = c(-1,1,-1,1) # natural, strategy, natural, strategy

pgamRoundxCondition = 1/(1+exp(-1*( (conditionBeta*conditions) + (roundBeta*rounds) + (condRoundIntxnBeta*conditions*rounds) )));

# natural, round 1 pgam = 0.4879496
# natural, round 2  pgam = 0.4671939
# strategy, round 1 pgam = 0.5075912
# strategy, round 2 pgam = 0.5372451

pdf(file.path(config$path$directory, config$path$shlab_figures,'pgamRoundxCondition.pdf'))
#par(pty="s")

plot(c(pgamRoundxCondition[1], pgamRoundxCondition[3]),  ylim = c(.4,.6), pch=16, axes = F, lwd = 5, col="goldenrod1", ylab="P(gamble)", xlab="Round", cex = 2)
points(c(pgamRoundxCondition[2], pgamRoundxCondition[4]), pch=16, cex=2, col="tomato1")
axis(1, at=c(1,2), label=c("1", "2"), lwd=6, cex=1.5)
axis(2, lwd =6, cex=1.5)
abline(a=.5, b=0, col="darkgrey", lty="dashed", lwd=1)

legend("bottomright", legend=c("Act Natural", "Strategy"),pch =16, bty="n", col=c("goldenrod1","tomato1"), cex=1)
dev.off()
```




```{r save-predicted-values, echo=FALSE}
rdmDFclean$predModel1= predict(model1_trialLev,type="link"); 


rdmDFclean$predModel3= predict(model3_trialLevRoundStrat,type="link"); 

# if you plot both of these predicted values, it it looks like there is a SLIGHT bias of unexplained gambling.

```


### Recent event models
#### glmer models are showing up as singular - we will use glm, effects are nearly identical between glmer and glm versions
```{r basic-recent-events-model}

# what happens when we account for strategy and round using predicted values from both of our trial level models
# using predicted values from model 1 above (gain, safe, ev)
model_roundStrat_pred1= glm(choice ~ 0 + roundRecode*strategyRecode, data=rdmDFclean, family="binomial", offset=predModel1)
summary(model_roundStrat_pred1)
# strategy is still significant and positive but no interaction betwen round and strategy

# using predicted avlues from model 3 above (gain, safe, ev, round and strategy)
model_roundStrat_pred3= glm(choice ~ 0 + roundRecode*strategyRecode, data=rdmDFclean, family="binomial", offset=predModel3)
summary(model_roundStrat_pred3)
# no effects of round, strategy or interaction


# PAST OUTCOME
model_poc_pred1 = glm(choice ~ 0 + pastOC1sc, data=rdmDFclean, family="binomial", offset=predModel1)
summary(model_poc_pred1)
# no outcome effect, negative beta is trending at <.1

model_poc_pred3 = glm(choice ~ 0 + pastOC1sc, data=rdmDFclean, family="binomial", offset=predModel3)
summary(model_poc_pred3)
# no outcome effect, negative beta is trending at <.1
# lower AIC than model above
# results are very similar as above


# PAST OUTCOME INTXN WITH STRATEGY
model_pocStrat_pred1 = glm(choice ~ 0 + pastOC1sc*strategyRecode, data=rdmDFclean, family="binomial", offset=predModel1)
summary(model_pocStrat_pred1)
# positive trending interaction between past outcome and strategy but no main effects of outcome (trending at p=.0962) or strategy
# act natural large past outcome = 1*-.07 + -1*.02 + (1*-1*.08) = -.17
# strategy large past outcome = 1*-.07 + 1*.02 + (1*1*.08) = .03
#  when told to ignore back outcome, the weight of previous outcome is not negative and switches to positive (and is weaker).
# how to establish that past outcome effect is not present or weaker in strategy. This would involve rewriting regressor.With strategy coded as -1 and 1, we are testing a difference between the conditions. Here we seee that there is meaningful differences in past otucome effect in natural vs strategy. To see whether there is an effect in one condition vs the other condiiton, then we code strategy as 1 (natural) and 0 (strategy) or the other way around. Or we take past outcome regressor and separate where its past outcome natural and past outcome strategy. In this case we can't compare the two conditions against each other.
# could also run separate regressions on different data piles.

model_pocStrat_pred3 = glm(choice ~ 0 + pastOC1sc*strategyRecode, data=rdmDFclean, family="binomial", offset=predModel3)
summary(model_pocStrat_pred3)

# similar results above with no interaction between outcome and strategy
# AIC lower for this model


# SHIFT
model_signedShiftStrat_pred1 = glm(choice ~ 0 + signedShiftsc*strategyRecode, data=rdmDFclean, family="binomial", offset=predModel1)
summary(model_signedShiftStrat_pred1)
# AIC: 24519
# positive effect of signed shift on risk-taking (more risk-taking as shift increases, less risk-taking a shift decreases)
# positive effect of strategy (more risk-taking in strategy condition)
# positive interaction 
    # how does the weight change across conditions and shift sizes:
    # strategy + large positive shift: .33*.59 + 1*.05 + .33*1*.45 = .3932
    # strategy + large negative shift: -.33*.59 + 1*.05 + -.33*1*.45 = -.2932
    # natural + large positive shift: .33*.59 + -1*.05 + .33*-1*.45 = -.0038
    # natural + large negative shift: -.33*.59 + -1*.05 + -.33*-1*.45 = -.0962
    # effect of shift (in both directions) is stronger in strategy condition

#the effect of positive shift is super dependent on strategy (there could be an overall effect of positive shift) but the interaction is important. Negative shift has a stronger overall effect and no interaction with strategy 


model_signedShiftStrat_pred3 = glm(choice ~ 0 + signedShiftsc*strategyRecode, data=rdmDFclean, family="binomial", offset=predModel3)
summary(model_signedShiftStrat_pred3)

# AIC lower than model above
# positive effct of signed shift (more risk-taking as shift increases)
# positive interaction between shift and strategy, similar as above
# no effect of strategy in this model (because we already accounted for it in trial-level?)

#POSITIVE AND NEGATIVE SHIFT
model_posNegShift_pred1 = glm(choice ~ 0 + posShiftsc  + negShiftsc, data=rdmDFclean, family="binomial", offset=predModel1)
summary(model_posNegShift_pred1) 
# only positive effect of negative shift

model_posNegShift_pred3 = glm(choice ~ 0 + posShiftsc  + negShiftsc, data=rdmDFclean, family="binomial", offset=predModel3)
summary(model_posNegShift_pred3)  
# similar results as above, just positive effect of negative shift (which ends up being less risk-taking with larger negative shifts)
# AIC smaller in this model


# POSITIVE AND NEGATIVE SHIFT WITH STRATEGY

model_posNegShiftStrat_pred1 = glm(choice ~ 0 + posShiftsc*strategyRecode + negShiftsc*strategyRecode, data=rdmDFclean, family="binomial", offset=predModel1)
summary(model_posNegShiftStrat_pred1) # AIC: 24519 (very similar to signedShift model)
# more risk-taking in strategy condition
# less risk-taking as negative shift increases (no interaction between negative shift and strategy)
# no main effect of positive shift but there is an interaction between positive shift and strategy
    # act natural, large positive shift = -1*.04 + .33*.43 + -1*.33*.88 = -0.1885
    # strategy, large positive shift = 1*.04 + .33*.43 + 1*.33*.88 = 0.4723
    # weight of positive shift on risk-taking is negative in natural condition
    # weight of positive shift on risk-taking is positive in strategy condition

model_posNegShiftStrat_pred3 = glm(choice ~ 0 + posShiftsc*strategyRecode + negShiftsc*strategyRecode, data=rdmDFclean, family="binomial", offset=predModel3)
summary(model_posNegShiftStrat_pred3)
# 24507 - these models with positive and negative shift (vs signed shift) are allowing asymmetries between the positive and negative shift effect 
# similar results above but with no main effect of strategy 
# similar negative effect of negative shift (beta is positive but neg shfit values are negative)
# and similar interaction between positive shift and strategy
# AIC smaller here

#shift effect is comparing current vs previous shift but when we tell people to ignore recent things, they aren't thinking about current values (and how it changes from previous trial) as contextual effects. Perhaps is because its not accesible and so people can't defend against it in a way they could with past outcome.

# EARNINGS X EXPECTATIONS (I.E. TRIAL NORMALIZED)

model_earnsExp_pred1 = glm(choice ~ 0 + earnNormalizedOverall + trialSC, data=rdmDFclean, family="binomial", offset=predModel1)
summary(model_earnsExp_pred1)
# betas are in direction as expected, earnings trending at p<.1

model_earnsExp_pred3 = glm(choice ~ 0 + earnNormalizedOverall + trialSC, data=rdmDFclean, family="binomial", offset=predModel3)
summary(model_earnsExp_pred3)
# no significant effects
# AIC smaller here

# EARNINGS AND EXPECTATIONS (TIME) INTERACTING
model_earnsExpIntxn_pred1 = glm(choice ~ 0 + earnNormalizedOverall*trialSC, data=rdmDFclean, family="binomial", offset=predModel1)
summary(model_earnsExpIntxn_pred1)
# no main effect of earnings
# negative effect of expectation 
# trending positive interaction p = .067

model_earnsExpIntxn_pred3 = glm(choice ~ 0 + earnNormalizedOverall*trialSC, data=rdmDFclean, family="binomial", offset=predModel3)
summary(model_earnsExpIntxn_pred3)
# same as model above
# aic smaller here


#EARNINGS, EXPECTATIONS AND STRATEGY
model_earnsExpIntxn_pred1 = glm(choice ~ 0 + earnNormalizedOverall*trialSC*strategyRecode, data=rdmDFclean, family="binomial", offset=predModel1)
summary(model_earnsExpIntxn_pred1)
# expectation significant and positive
# trending interaction between earnings and expectaions


model_earnsExpIntxn_pred3 = glm(choice ~ 0 + earnNormalizedOverall*trialSC*strategyRecode, data=rdmDFclean, family="binomial", offset=predModel3)
summary(model_earnsExpIntxn_pred3)
# similar results as above
# AIC smaller


#check earnings and expectations overall - breaking up earnings could be hurting us here.
#could still interact with strategy

# EARNINGS AND OUTCOME INTERACTION

model_earnsOcExpIntxn_pred1 = glm(choice ~ 0 + earnNormalizedOverall*pastOC1sc*trialSC*strategyRecode, data=rdmDFclean, family="binomial", offset=predModel1)
summary(model_earnsOcExpIntxn_pred1)
# negative effect of past outcome
# positive interaction between earnings and outcome and negative interaction bewteen expectations and outcome
# AIC: 24318

# model above but remove interaction that includes trial
model_earnsOcIntxn_pred1 = glm(choice ~ 0 + earnNormalizedOverall*pastOC1sc*strategyRecode + trialSC, data=rdmDFclean, family="binomial", offset=predModel1)
summary(model_earnsOcIntxn_pred1)
# negative effect of past outcome
# trending positive effect of earnings
# positive interaction between earnings and outcome
# no effect of expectation now
# AIC =  24316 this is lower than above when including expectation in interaction 

model_earnsOcIntxn_pred3 = glm(choice ~ 0 + earnNormalizedOverall*pastOC1sc*strategyRecode + trialSC, data=rdmDFclean, family="binomial", offset=predModel3)
summary(model_earnsOcIntxn_pred3)
# results are similar as above
# AIC lower

# PUT ALL TIMESCALES IN MODEL

model_3timescales_pred1 = glm(choice ~ 0 + pastOC1sc*earnNormalizedOverall + posShiftsc + negShiftsc + trialSC,  data=rdmDFclean, family="binomial", offset=predModel1)
summary(model_3timescales_pred1)
# negative past outcome effect
# effect of negative shift only
# interaction between past outcome and earnings 
# trending positive effect of earnings

model_3timescales_pred3 = glm(choice ~ 0 + pastOC1sc*earnNormalizedOverall + posShiftsc + negShiftsc + trialSC,  data=rdmDFclean, family="binomial", offset=predModel3)
summary(model_3timescales_pred3)
# similar as model above = 24300
# negative past outcome effect
# effect of negative shift only
# positive interaction between past outcome and earnings 
# trending main effects of earnings and expectations
# smaller AIC here

# If you add an interaction between outcomes and expectations to this model, the interaction between past outcome and earnings and past outcome and expectation shows up.
# AIC: 24293 this is lower than when we don't add the interaction with poc and expectations

model_3timescalesStrategy_pred1 = glm(choice ~ 0 + pastOC1sc*earnNormalizedOverall*strategyRecode + posShiftsc*strategyRecode + negShiftsc*strategyRecode + trialSC*strategyRecode, data=rdmDFclean, family="binomial", offset=predModel1)
# similar main effects as above include negative past outcome, negative shift effect, pocxearnings
# strategy only effects positive shift
summary(model_3timescalesStrategy_pred1)

model_3timescalesStrategy_pred3 = glm(choice ~ 0 + pastOC1sc*earnNormalizedOverall*strategyRecode + posShiftsc*strategyRecode + negShiftsc*strategyRecode + trialSC*strategyRecode, data=rdmDFclean, family="binomial", offset=predModel3)

summary(model_3timescalesStrategy_pred3)
# AIC = 24298
# similar main effects as above include negative past outcome, negative shift effect, pocxearnings
# this models hows additional negative effect of strategy
# similar positive interaction between shift and strategy as above
# AIC is 17841 - similar as above

# are we weakening the cumulative earnings effect by cutting it off between rounds? Do people track earnings over rounds?
model_3timescalesStrategy_earnOverall_pred3 = glm(choice ~ 0 + pastOC1sc*earningsAcrossRounds*strategyRecode + posShiftsc*strategyRecode + negShiftsc*strategyRecode + trialSC*strategyRecode, data=rdmDFclean, family="binomial", offset=predModel3)

summary(model_3timescalesStrategy_earnOverall_pred3)
# 24304 - a little higher than above
# negative past outcome and strategy effect.
# trending positive effect of earnings
# negative effect of negative shift (beta is positive but the effect is less risk-taking)
# negative trending effect of trial/expectations
# positive interaction between poc and earnings across rounds
# positive interaction bewteen positive shift and strategy

# and including trial over all
model_3timescalesStrategy_earnTrialOverall_pred3 = glm(choice ~ 0 + pastOC1sc*earningsAcrossRounds*strategyRecode + posShiftsc*strategyRecode + negShiftsc*strategyRecode + trialAcrossRounds*strategyRecode, data=rdmDFclean, family="binomial", offset=predModel3)

summary(model_3timescalesStrategy_earnTrialOverall_pred3)
# 24307
# similar as model above but now we see main effects of earnings and trial in the expected directions

# the best fitting model is one that accounts for earnings across both rounds but not expectations/trial across both rounds but when accounting for earnings and trial across rounds, we see the effects we'd expect to see.

# between subjects, it doesn't look like earnings and expectations don't interact with strategy at this point. 
# the strategy effect does seem to enhance one timescale but doesn't change the other two 
# if people ask: people who repeat conditions - we don't see an effect of strategy on earnings vs trial doing a between group analysis. things are getting complicated with people who change and these variables are cumulative but with a potential reset in the middle. 



```


```{r}
# it looks like people who repeat conditions may be more likely to track earnings over the rounds whereas people who switch strategies might reset between rounds

# subset data for those who were in condition code 1 and 4
repeatconditions = c(condcode1, condcode4)
rdmDFrepeatCond = rdmDFclean[as.numeric(rdmDFclean$subID) %in% repeatconditions,]

model1_repeatcond_trialLevRoundStrat= glmer(choice ~ 1 + gainScaled + safeScaled + evLevScaled + roundRecode*strategyRecode + (1|subID), data=rdmDFrepeatCond , family = "binomial")
summary(model1_repeatcond_trialLevRoundStrat)

# get predicted values
rdmDFrepeatCond$pred= predict(model1_repeatcond_trialLevRoundStrat,type="link"); 


model_repeatcond_3timescalesStrategy = glm(choice ~ 0 + pastOC1sc*earnNormalizedOverall*strategyRecode + posShiftsc*strategyRecode + negShiftsc*strategyRecode + trialSC*strategyRecode, data=rdmDFrepeatCond, family="binomial", offset=pred)
summary(model_repeatcond_3timescalesStrategy)
#  11727


model_repeatcond_3timescalesStrategy_earnExpAcrossRounds = glm(choice ~ 0 + pastOC1sc*earningsAcrossRounds*strategyRecode + posShiftsc*strategyRecode + negShiftsc*strategyRecode + trialAcrossRounds*strategyRecode, data=rdmDFrepeatCond, family="binomial", offset=pred)
summary(model_repeatcond_3timescalesStrategy_earnExpAcrossRounds)
# 11723




# subset data for those who were in condition code 2 and 3
switchconditions = c(condcode2, condcode3)
rdmDFswitchCond = rdmDFclean[as.numeric(rdmDFclean$subID) %in% switchconditions,]

model1_switchcond_trialLevRoundStrat= glmer(choice ~ 1 + gainScaled + safeScaled + evLevScaled + roundRecode*strategyRecode + (1|subID), data=rdmDFswitchCond , family = "binomial")
summary(model1_switchcond_trialLevRoundStrat)

# get predicted values
rdmDFswitchCond$pred= predict(model1_switchcond_trialLevRoundStrat,type="link"); 


model_switchcond_3timescalesStrategy = glm(choice ~ 0 + pastOC1sc*earnNormalizedOverall*strategyRecode + posShiftsc*strategyRecode + negShiftsc*strategyRecode + trialSC*strategyRecode, data=rdmDFswitchCond, family="binomial", offset=pred)
summary(model_switchcond_3timescalesStrategy)
# 12541


model_switchcond_3timescalesStrategy_earnExpAcrossRounds = glm(choice ~ 0 + pastOC1sc*earningsAcrossRounds*strategyRecode + posShiftsc*strategyRecode + negShiftsc*strategyRecode + trialAcrossRounds*strategyRecode, data=rdmDFswitchCond, family="binomial", offset=pred)
summary(model_switchcond_3timescalesStrategy_earnExpAcrossRounds)
# 12552




```



```{r, plot-big-model-results}
# haven't updated this
# first lets plot the effect of past outcome and strategy
# from big model, accounting for earnings and expectations across rounds using residuals model 3
pocBeta = -0.31592
strategyBeta = -0.12298
pocXstrategyBeta= 0.30742

poc = rep(seq(0,1, by=.1), times= 2)
strategy = rep(c(-1,1), each = length(poc)/2)
  
pgamPOCxStrategy= 1/(1+exp(-1*( (pocBeta*poc) + (strategyBeta*strategy) + (pocXstrategyBeta*poc*strategy) )));


pdf(file.path(config$path$directory, config$path$shlab_figures,'pgamPOCxStrategy.pdf'))
par(pty="s")
plot(pgamPOCxStrategy[1:11], type="l", ylim=c(.35,.6), ylab="P(gamble)", xlab="Past Outcome ($)", lwd=5, col = "goldenrod1", axes = F)
lines(pgamPOCxStrategy[12:22], lwd=5, col="tomato1")
abline(a=.5, b=0, lwd=3, lty="dashed", col = "darkgrey")
lines(pgamPOCxStrategy[1:11], lwd=5, col="goldenrod1")
axis(1, at = c(1,6,11), labels = c(0,30,61), lwd=5, cex = 1.5)
axis(2, lwd = 5, cex = 1.5)

legend("topright", legend=c("Act Natural", "Strategy"), lty=c("solid", "solid"),lwd = 3, bty="n", col=c("goldenrod1","tomato1"), cex=1.15)
dev.off()

# for presentation purposes, just plot act natural line
pdf(file.path(config$path$directory, config$path$shlab_figures,'pgamPOCxStrategy_actNatLineOnly.pdf'))
par(pty="s")
plot(pgamPOCxStrategy[1:11], type="l", ylim=c(.35,.6), ylab="P(gamble)", xlab="Past Outcome ($)", lwd=5, col = "goldenrod1", axes = F)
#lines(pgamPOCxStrategy[12:22], lwd=5, col="tomato1")
abline(a=.5, b=0, lwd=3, lty="dashed", col = "darkgrey")
lines(pgamPOCxStrategy[1:11], lwd=5, col="goldenrod1")
axis(1, at = c(1,6,11), labels = c(0,30,61), lwd=5, cex = 1.5)
axis(2, lwd = 5, cex = 1.5)

legend("topright", legend=c("Act Natural", "Strategy"), lty=c("solid", "solid"),lwd = 3, bty="n", col=c("goldenrod1","tomato1"), cex=1.15)
dev.off()


# plot shift effect
posShiftBeta = 0.44823
negShiftBeta = 1.31489
posShiftStratIntxn = 1.15739
negShiftStratIntxn = 0.17565

posShiftAmts = rep(seq(0, .33, length.out = 3), times =2)
negShiftAmts = rep(seq(-.33, 0, length.out = 3), times = 2)
strategy = rep(c(-1,1), each = length(posShiftAmts)/2)

pgamPosshiftxStrategy= 1/(1+exp(-1*( (posShiftBeta*posShiftAmts) + (strategyBeta*strategy) + (posShiftStratIntxn*posShiftAmts*strategy) )));

pgamNegShiftxStrategy = 1/(1+exp(-1*( (negShiftBeta*negShiftAmts) + (strategyBeta*strategy) + (negShiftStratIntxn*negShiftAmts*strategy) )));

pdf(file.path(config$path$directory, config$path$shlab_figures,'pgamPosShiftxStrategy.pdf'))
par(pty="s")
plot(pgamPosshiftxStrategy[1:3], type="l", ylim=c(.4,.6), ylab="P(gamble)", xlab="Positive Shift ($)", lwd=5, col = "goldenrod1", axes = F)
lines(pgamPosshiftxStrategy[4:6], lwd=5, col="tomato1")
abline(a=.5, b=0, lwd=3, lty="dashed", col = "darkgrey")
lines(pgamPosshiftxStrategy[1:3], lwd=5, col="goldenrod1")
lines(pgamPosshiftxStrategy[4:6], lwd=5, col="tomato1")
axis(1, at = c(1:3), labels = c(0,10,20), lwd=5, cex = 1.5)
axis(2, lwd = 5, cex = 1.5)

legend("bottomright", legend=c("Act Natural", "Strategy"), lty=c("solid", "solid"),lwd = 3, bty="n", col=c("goldenrod1","tomato1"), cex=1.15)
dev.off()



pdf(file.path(config$path$directory, config$path$shlab_figures,'pgamPosShiftxStrategy_actNatLineOnly.pdf'))
par(pty="s")
plot(pgamPosshiftxStrategy[1:3], type="l", ylim=c(.4,.6), ylab="P(gamble)", xlab="Positive Shift ($)", lwd=5, col = "goldenrod1", axes = F)
#lines(pgamPosshiftxStrategy[4:6], lwd=5, col="tomato1")
abline(a=.5, b=0, lwd=3, lty="dashed", col = "darkgrey")
lines(pgamPosshiftxStrategy[1:3], lwd=5, col="goldenrod1")
#lines(pgamPosshiftxStrategy[4:6], lwd=5, col="tomato1")
axis(1, at = c(1:3), labels = c(0,10,20), lwd=5, cex = 1.5)
axis(2, lwd = 5, cex = 1.5)

#legend("bottomright", legend=c("Act Natural", "Strategy"), lty=c("solid", "solid"),lwd = 3, bty="n", col=c("goldenrod1","tomato1"), cex=1.15)
dev.off()



pdf(file.path(config$path$directory, config$path$shlab_figures,'pgamNegShiftxStrategy.pdf'))
par(pty="s")
plot(pgamNegShiftxStrategy[1:3], type="l", ylim=c(.35,.6), ylab="P(gamble)", xlab="Negative Shift ($)", lwd=5, col = "goldenrod1", axes = F)
lines(pgamNegShiftxStrategy[4:6], lwd=5, col="tomato1")
abline(a=.5, b=0, lwd=3, lty="dashed", col = "darkgrey")
lines(pgamNegShiftxStrategy[1:3], lwd=5, col="goldenrod1")
lines(pgamNegShiftxStrategy[4:6], lwd=5, col="tomato1")
axis(1, at = c(1:3), labels = c(-20,-10,0), lwd=5, cex = 1.5)
axis(2, lwd = 5, cex = 1.5)

legend("bottomright", legend=c("Act Natural", "Strategy"), lty=c("solid", "solid"),lwd = 3, bty="n", col=c("goldenrod1","tomato1"), cex=1.15)
dev.off()


```



```{r, subset-round1-data}

rdmRound1df = rdmDFclean[rdmDFclean$roundRDM==1,]

natRound1 = rcsSubLevelWide_clean[rcsSubLevelWide_clean$round1_strategy==0,]
stratRound1 = rcsSubLevelWide_clean[rcsSubLevelWide_clean$round1_strategy==1,]


# more risk-taking in strategy condition
mean(natRound1$round1_pgamble)
mean(stratRound1$round1_pgamble)

# slightly slower in strategy 
mean(natRound1$round1_RT)
mean(stratRound1$round1_RT)

# more difficult in strategy
mean(natRound1$round1_instDifficult)
mean(stratRound1$round1_instDifficult)

# instruction frequency higher in strategy
mean(natRound1$round1_instHowOften)
mean(stratRound1$round1_instHowOften)
t.test(natRound1$round1_instHowOften, stratRound1$round1_instHowOften, paired = FALSE) # not a meaningful difference



model_round1_triLevelStrat = glmer(choice ~ 1 + gainScaled + safeScaled + evLevScaled + strategyRecode + (1|subID), data=rdmRound1df , family = "binomial")
summary(model_round1_triLevelStrat)
# gain, safe and ev as expected
# no effect of strategy

rdmRound1df$predRnd1TriLev= predict(model_round1_triLevelStrat,type="link"); 

model_round1_3timescales = glm(choice ~ 0 + pastOC1sc*earnNormalizedOverall*strategyRecode + posShiftsc*strategyRecode + negShiftsc*strategyRecode + trialSC*strategyRecode, data=rdmRound1df, family="binomial", offset=predRnd1TriLev)
summary(model_round1_3timescales)
# similar results above even when just looking at round 1 data
  # negative past outcome, negative shift effect, interaction between strategy and positive shift
  # trending positive effect of earnings




```


```{r explore-individual-differences}

# is there a relationship between strategy instruction difficulty and ospan/symspan scores
# start by just looking at round 1


cor.test(stratRound1$round1_instDifficult, stratRound1$ospan, method="spearman"); 
cor.test(stratRound1$round1_instDifficult, stratRound1$symspan, method="spearman"); 
cor.test(stratRound1$round1_instDifficult, stratRound1$ERQsupp, method="spearman"); 
cor.test(stratRound1$round1_instDifficult, stratRound1$ERQreapp, method="spearman"); 
# no correlation between instruction difficulty for participants who did strategy first and complex span or ERQ scores


cor.test(stratRound1$round1_instHowOften, stratRound1$ospan, method="spearman"); 
cor.test(stratRound1$round1_instHowOften, stratRound1$symspan, method="spearman"); 
cor.test(stratRound1$round1_instHowOften, stratRound1$ERQsupp, method="spearman"); # positive correlation
cor.test(stratRound1$round1_instHowOften, stratRound1$ERQreapp, method="spearman"); 

# no correlation between instruction frequency for participants who did strategy first with ospan, symspan, and ERQ reappraisal but there is a weak, positive relationship between instruction frequency and ERQ supression. Could this mean that people who have a tendency to supress emotional experiences are better at implementing the strategy instructions more often?


# Are instruction difficulty related to how often participants rated following the instructions
cor.test(stratRound1$round1_instDifficult, stratRound1$round1_instHowOften, method="spearman"); # difficulty rating negatively correlated with frequency (more difficult, lower frequency ratings)




cor.test(natRound1$round1_instDifficult, natRound1$ospan, method="spearman"); 
cor.test(natRound1$round1_instDifficult, natRound1$symspan, method="spearman"); 
cor.test(natRound1$round1_instDifficult, natRound1$ERQsupp, method="spearman"); 
cor.test(natRound1$round1_instDifficult, natRound1$ERQreapp, method="spearman"); 
# no correlation between instruction difficulty for participants who did natural first and complex span or ERQ scores


cor.test(natRound1$round1_instHowOften, natRound1$ospan, method="spearman"); # positive correlation
cor.test(natRound1$round1_instHowOften, natRound1$symspan, method="spearman"); 
cor.test(natRound1$round1_instHowOften, natRound1$ERQsupp, method="spearman"); 
cor.test(natRound1$round1_instHowOften, natRound1$ERQreapp, method="spearman"); 

# no correlation between instruction frequency for participants who did natural first with symspan, and ERQ scores but there is a positive relationship between instruction frequency and ospan. 


# Are instruction difficulty related to how often participants rated following the instructions
cor.test(natRound1$round1_instDifficult, natRound1$round1_instHowOften, method="spearman"); # difficulty rating negatively correlated with frequency (more difficult, lower frequency ratings)


# does strategy interact with ospan and symspan
# not sure if it makes sense to do this but:
model_3timescalesStrategy_complexSpan_pred3 = glm(choice ~ 0 + pastOC1sc*strategyRecode*symSpanScore + posShiftsc*strategyRecode*symSpanScore  + earnNormalizedOverall*strategyRecode*symSpanScore +  trialSC*strategyRecode*symSpanScore, data=rdmDFclean, family="binomial", offset=predModel3)
summary(model_3timescalesStrategy_complexSpan_pred3); # AIC: 20943; 3570 observations deleted (no data for those participants)
# positive interaction bewteen poc and strategy (strategy makes poc effect less negative and maybe even goes away but haven't plotted that yet)
# trending positive interaction between strategy and positive shift (making positive shift stronger)
# negative interaction between poc, strategy and symspan: 
# large past outcome, strategy condition, high symspan: -.08*(1) -.07*(1) +  0.003*(14) -0.035*(1)*(1)*(14) = -0.598
# large past outcome, strategy condition, low symspan: -.08*(1) -.07*(1) +  0.003*(0) -0.035*(1)*(1)*(0) = -.15

# large past outcome, natural condition, high symspan: -.08*(1) -.07*(1) +  0.003*(14) -0.035*(1)*(1)*(14) = -0.598
# large past outcome, natural condition, low symspan: -.08*(1) -.07*(1) +  0.003*(0) -0.035*(1)*(1)*(0) = -.15


# plot intxn between symspan, poc and strategy effect
pocBeta = -.08
stratBeta = -.07
symspanBeta = 0.003
pocStratSymspanBeta = -0.035

pocAmt = rep(seq(0, 1, length.out = 2), times =4)
symspanAmt = rep(c(0,14, 0,14), each= 2)
strategy = rep(c(-1,1), each = length(pocAmt)/2)

pgamPOCstrategySymspan= 1/(1+exp(-1*( (pocBeta*pocAmt) + (stratBeta*strategy) + (symspanBeta*symspanAmt) + (pocStratSymspanBeta*pocAmt*symspanAmt * strategy) )));


# 0 outcome, low symspan, natural: 0.5174929
# 0 outcome, high symspan, natural: 0.5279708
# big outcome, low symspan, natural: 0.4975000
# big outcome, high symspan, natural: 0.6276153


# 0 outcome, low symspan, strategy: 0.4825071
# 0 outcome, high symspan, strategy: 0.4930005
# big outcome, low symspan, strategy: 0.4625702 
# big outcome, high symspan, strategy: 0.3548014
# hard to really get what is going on here but it seems like higher sympspan is associated with stronger impact of past outcome in strategy condition


model_3timescalesStrategy_oSpan_pred3 = glm(choice ~ 0 + pastOC1sc*strategyRecode*ospanScore + posShiftsc*strategyRecode*ospanScore  + earnNormalizedOverall*strategyRecode*ospanScore +  trialSC*strategyRecode*ospanScore, data=rdmDFclean, family="binomial", offset=predModel3)
summary(model_3timescalesStrategy_oSpan_pred3); # AIC: 22244; 2014 observations deleted (no data for those participants)
# there's more data in the ospan model so the AIC will be higher but ospan doesn't interact with anything


# let's take a step back and just look at whether difficulty and frequency ratings of instrcutions are related to symspan and ERQ using glm

rcsSubLevelLong_clean$strategyRecode = rcsSubLevelLong_clean$strategy;
rcsSubLevelLong_clean$strategyRecode[rcsSubLevelLong_clean$strategyRecode==0] = -1; # change 0s to -1

instDiff_model1 = lmer(instDifficult ~ 1+ ospan + symspan + ERQreapp +  ERQsupp + strategyRecode + (1|subID), data=rcsSubLevelLong_clean)
summary(instDiff_model1)
# difficulty ratings significantly higher in strategy condition
# no relationship between instruction difficulty and working memory/ERQ

# in a model interacting strategy with complex span and ERQ - no interaction between strategy with the other things; and now main effect of  strategy goes away - asking too much of model?


instHowOften_model1 = lmer(instHowOften ~ 1+ ospan + symspan + ERQreapp +  ERQsupp + strategyRecode + (1|subID), data=rcsSubLevelLong_clean)
summary(instHowOften_model1)
# positive relationship between instructions frequency and ospan - higher ospan, higher frequency rating
# no effect of strategy on frequency

instHowOften_model2 = lmer(instHowOften ~ 1+ ospan*strategyRecode + symspan*strategyRecode + ERQreapp*strategyRecode +  ERQsupp*strategyRecode+  (1|subID), data=rcsSubLevelLong_clean)
summary(instHowOften_model2)
# no interactions with strategy or main effect of strategy.

```

